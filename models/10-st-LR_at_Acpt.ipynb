{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, auc, f1_score, explained_variance_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, '../src/visualization/')\n",
    "import visualize as vis\n",
    "\n",
    "df = pd.read_csv('../data/interim/Third_order_clean_confidential.csv').drop(columns=['Unnamed: 0'])\n",
    "contact_codes = pd.read_csv('../data/processed/contact_codes_at_application.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge new and old SAT scores, and drop the SATWriting category.\n",
    "\n",
    "\n",
    "List of columns worth 1-hot-encoding:\n",
    "* Application_Type\n",
    "* Application_Format\n",
    "* Region\n",
    "* Dependent/Independent\n",
    "* Gender\n",
    "* Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['State_grants','Pell_grant', 'HSCEEB', 'ADMT_DEC_CODE',\n",
    "                     'Ccbnm_long','Ccbnm_lat','Dist_to_Ccbnm','Home_Long',\n",
    "                    'Home_Lat','Number_of_campus_visits','TotalWeight',\n",
    "                     'Fresh_enroll', 'Year_of_entry','FT_Tuition_Fees','HS_Numeric_rank','WeightatAcpt',\n",
    "                    'Selected_for_verification','Ccbnm_for_dist','College_chosen_by_non-matrics',\n",
    "                     'DOB','SAT_combined','SAT_reading','SAT_writing','SAT_math','NEWSATVerbal',\n",
    "                     'NEWSATVerbalMath','NEWSATMath','Street1_perm_res','Zip_perm_res','Major',\n",
    "                     'Test_Optional','Cond_Admit','Dependent_Independent_status','Verification_Completed',\n",
    "                     'Initial_inquiry_source','First_generation','Status','Indicated_intent_to_apply_for_FA',\n",
    "                     'Admission_status','County_perm_res','City_perm_res','State_perm_res','International_student',\n",
    "                     'HD_Academic_Rating','FatherEd','MotherEd','Application_Type','Application_format',\n",
    "                     'COA','RESD','COMM','Gender'\n",
    "                    ])\n",
    "\n",
    "for col in X.columns.values:\n",
    "    for word in ['FM','Wages','Cash','worth','investment','budget','date','FAFSA','coa','Ints',\n",
    "                'cost','income']:\n",
    "        if word.lower() in col.lower():\n",
    "            X = X.drop(columns=col)\n",
    "            break\n",
    "            \n",
    "col_names = ['Region_'+region for region in ['A','B','C','D','E','F','G','H','I','J','K','Z']]\n",
    "X[col_names] = pd.get_dummies(X['Region'])[['A','B','C','D','E','F','G','H','I','J','K','Z']]\n",
    "X = X.drop(columns='Region')\n",
    "\n",
    "# col_names = ['Male','Female','Non-Binary']\n",
    "# X[col_names] = pd.get_dummies(X['Gender'])\n",
    "# X = X.drop(columns='Gender')\n",
    "\n",
    "# for col in contact_codes.columns.values:\n",
    "#     if col!='Unique_student_ID':\n",
    "#         X[col] = X['Unique_student_ID'].map(contact_codes['Unique_student_ID'],contact_codes[col])\n",
    "\n",
    "X = pd.merge(X,contact_codes,how='left',on='Unique_student_ID')\n",
    "\n",
    "X = X.drop(columns='Unique_student_ID').drop_duplicates()\n",
    "\n",
    "Y = X['Enrolled'].fillna(-999)\n",
    "\n",
    "X = X.drop(columns=\"Enrolled\").fillna(-999)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data before PCA\n",
    "from sklearn.preprocessing import scale\n",
    "scaled = pd.DataFrame(scale(X),columns=X.columns.values)\n",
    "\n",
    "# implementing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5).fit(scaled)\n",
    "pca_samples = pca.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension indexing\n",
    "dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "# PCA components\n",
    "components = pd.DataFrame(np.round(pca.components_, 4), columns = scaled.keys()) \n",
    "components.index = dimensions\n",
    "\n",
    "# PCA explained variance\n",
    "ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1) \n",
    "variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance']) \n",
    "variance_ratios.index = dimensions\n",
    "\n",
    "# Create a bar plot visualization\n",
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "# Plot the feature weights as a function of the components\n",
    "components.plot(ax = ax, kind = 'bar')\n",
    "ax.set_ylabel(\"Feature Weights\") \n",
    "ax.set_xticklabels(dimensions, rotation=0)\n",
    "\n",
    "# Display the explained variance ratios# \n",
    "for i, ev in enumerate(pca.explained_variance_ratio_): \n",
    "    ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n %.4f\"%(ev))\n",
    "ax.get_legend().remove()\n",
    "    \n",
    "pca_results = pd.concat([variance_ratios, components], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_explained_variance_ratio(pca);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3).fit(scaled)\n",
    "reduced_data = pca.transform(scaled)\n",
    "\n",
    "pca_samples = pca.transform(scaled)\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2','Dimension 3'])\n",
    "\n",
    "red_enrolled = pd.merge(reduced_data,Y,how='left',left_index=True,right_index=True).drop_duplicates()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "# scatterplot of the reduced data \n",
    "ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], c=Y, s=20, alpha=0.5)\n",
    "\n",
    "feature_vectors = pca.components_.T\n",
    "\n",
    "# using scaling factors to make the arrows\n",
    "arrow_size, text_pos = 7.0, 8.0,\n",
    "\n",
    "# projections of the original features\n",
    "for i, v in enumerate(feature_vectors):\n",
    "    ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], head_width=0.2, head_length=0.2, linewidth=2, color='red')\n",
    "    ax.text(v[0]*text_pos, v[1]*text_pos, X.columns[i], color='black', ha='center', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel(\"Dimension 1\", fontsize=14)\n",
    "ax.set_ylabel(\"Dimension 2\", fontsize=14)\n",
    "ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n",
    "\n",
    "plt.xlim(-5,10);\n",
    "\n",
    "print(\"# Applicants in bottom left group: %d\" % len(red_enrolled.where((red_enrolled['Dimension 1']<2) & (red_enrolled['Dimension 2']<0)).dropna()))\n",
    "print(\"# Enrolled in bottom left group: %d \\n\" % len(red_enrolled.where((red_enrolled['Dimension 1']<2) & (red_enrolled['Dimension 2']<0) & (red_enrolled['Enrolled'])).dropna()))\n",
    "\n",
    "print(\"# Applicants in bottom right group: %d\" % len(red_enrolled.where((red_enrolled['Dimension 1']>2) & (red_enrolled['Dimension 2']<0)).dropna()))\n",
    "print(\"# Enrolled in bottom right group: %d \\n\" % len(red_enrolled.where((red_enrolled['Dimension 1']>2) & (red_enrolled['Dimension 2']<0) & (red_enrolled['Enrolled'])).dropna()))\n",
    "\n",
    "print(\"# Applicants in upper left group: %d\" % len(red_enrolled.where((red_enrolled['Dimension 1']<2) & (red_enrolled['Dimension 2']>0)).dropna()))\n",
    "print(\"# Enrolled in upper left group: %d \\n\" % len(red_enrolled.where((red_enrolled['Dimension 1']<2) & (red_enrolled['Dimension 2']>0) & (red_enrolled['Enrolled'])).dropna()))\n",
    "\n",
    "print(\"# Applicants in upper right group: %d\" % len(red_enrolled.where((red_enrolled['Dimension 1']>2) & (red_enrolled['Dimension 2']>0)).dropna()))\n",
    "print(\"# Enrolled in upper right group: %d \\n\" % len(red_enrolled.where((red_enrolled['Dimension 1']>2) & (red_enrolled['Dimension 2']>0) & (red_enrolled['Enrolled'])).dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(model.coef_[0])\n",
    "feature_importance.index = X.columns.values\n",
    "\n",
    "feature_importance = (abs(feature_importance)).sort_values()\n",
    "\n",
    "f, axes = plt.subplots(figsize=(8,10))\n",
    "feature_importance.plot(kind='barh');\n",
    "plt.xlabel(\"Feature Weight\")\n",
    "plt.ylabel(\"Feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Likelihood of Attendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_of_attendance = y_pred[:,1] # first column is prob 0, second column is prob 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make DataFrame of Likelihood v. Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pd.DataFrame([list(likelihood_of_attendance),y_test]).T.rename(columns={0:\"Likelihood of Enrollment\",1:\"Enrolled\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(figsize=(20,12))\n",
    "sns.distplot(likelihood['Likelihood of Enrollment'][likelihood['Enrolled']],kde=False,bins=30)\n",
    "sns.distplot(likelihood['Likelihood of Enrollment'][likelihood['Enrolled']==False],kde=False,bins=30)\n",
    "\n",
    "plt.title(\"Chance of Enrollment\");\n",
    "vals = axes.get_xticks()\n",
    "axes.set_xticklabels(['{:,.0%}'.format(x) for x in vals]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good indicator for enrollment??? What about being a good indicator for students to *NOT* pursue??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at chance of enrollment > 20%, and likelihood < 5 %\n",
    "unlikely = likelihood.where(likelihood['Likelihood of Enrollment']<=0.05).dropna()\n",
    "more_likely = likelihood.where(likelihood['Likelihood of Enrollment']>=0.2).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlikely.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_likely.describe() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.8,\n",
    " 'learning_rate': 0.05,\n",
    " 'max_depth': 5,\n",
    " 'min_child_weight': 11,\n",
    " 'missing': -999,\n",
    " 'n_estimators': 500,\n",
    " 'nthread': 4,\n",
    " 'seed': 42,\n",
    " 'silent': 1,\n",
    " 'subsample': 0.8,\n",
    " 'objective': 'reg:logistic'}  # found from GridSearchCV \n",
    "\n",
    "modelX = xgb.XGBRegressor(**best_params)\n",
    "modelX.fit(X_train, y_train)\n",
    "y_pred = modelX.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.my_plot_importance(modelX,figsize=(9,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(figsize=(10,6))\n",
    "sns.distplot(y_pred,kde=False)\n",
    "plt.title(\"Chance of Enrollment\");\n",
    "vals = axes.get_xticks()\n",
    "axes.set_xticklabels(['{:,.0%}'.format(x) for x in vals]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = pd.DataFrame([y_pred,y_test]).T.rename(columns={0:\"Likelihood of Enrollment\",1:\"Enrolled\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlikely = likelihood.where(likelihood['Likelihood of Enrollment']<=0.05).dropna()\n",
    "more_likely = likelihood.where(likelihood['Likelihood of Enrollment']>=0.2).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlikely.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_likely.describe() # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Look at XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(modelX)\n",
    "shap_values = explainer.shap_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_plot = shap.summary_plot(shap_values, X, max_display=15, show=True, alpha=0.7,\n",
    "                  plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X, max_display=20, show=True,\n",
    "                  plot_type='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
