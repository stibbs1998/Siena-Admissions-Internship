{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Codes\n",
    "\n",
    "Trying to predict the likelihood of enrollment based solely on previous contacts with the college.\n",
    "These contacts include things such as: \n",
    "* campus visits\n",
    "* meeting at a college fair\n",
    "* talking with a recruiter at their high school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/visualization/')\n",
    "import visualize as vis\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Contact Codes Data Sets and the Application Data Set.\n",
    "\n",
    "The Contact Codes Data Set was altered such that all date-like attributes are recorded solely by the total days since 0 A.D.  Both files are included in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields of Interest:\n",
    "* Admission_application_date\n",
    "* Admission_date\n",
    "* Enrolled?\n",
    "* Unique_student_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dates = pd.read_csv('../data/processed/Qry_Contacts_xtab.csv')\n",
    "codes_days = pd.read_csv('../data/processed/Qry_Contacts_xtab_days.csv')\n",
    "\n",
    "df = pd.read_csv(\n",
    "    '../data/processed/CriticalPath_Data_EM_Confidential_lessNoise.csv').drop(columns='Unnamed: 0')[[\n",
    "    \"Unique_student_ID\",\"Enrolled\",\"Admission_application_date\",\"WeightatAcpt\",\"TotalWeight\"\n",
    "]]\n",
    "\n",
    "df['Admission_application_date'] = pd.to_datetime(df['Admission_application_date'])\n",
    "\n",
    "months_to_days = {\n",
    "1:0,\n",
    "2:31,\n",
    "3:59,\n",
    "4:90,\n",
    "5:120,\n",
    "6:151,\n",
    "7:181,\n",
    "8:212,\n",
    "9:243,\n",
    "10:273,\n",
    "11:304,\n",
    "12:334\n",
    "}\n",
    "\n",
    "\n",
    "df['Admission_application_date_asDays'] = df['Admission_application_date'].dt.day + df['Admission_application_date'].dt.month.map(months_to_days) + df['Admission_application_date'].dt.year*365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only the students that are in the `CriticalPath_Data` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_days = codes_days.where(\n",
    "    df.Unique_student_ID.isin(codes_days.Unique_student_ID)).dropna(subset=['Unique_student_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the files together such that whether or not a student enrolls is linkable to all of the contact codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(codes_days,df,how='left',on=\"Unique_student_ID\")\n",
    "\n",
    "for col in codes_days.columns.values:\n",
    "    if col!='Unique_student_ID':\n",
    "        data[col] = data[col][data[col] < data['Admission_application_date_asDays']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a shadow matrix of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_matrix = ~data.isna().drop(columns=[\"Unique_student_ID\",\"Admission_application_date\",\"Enrolled\",\"WeightatAcpt\",\"Admission_application_date_asDays\",\"TotalWeight\"]).astype(int)+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ned had previously said that the contact code weights in the WeightAtAccpt column were determined by the first letter of the contact code.\n",
    "\n",
    "Therefore, take the sum of interactions for each letter, and use these columns in place of the original contact codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in \"ABCDEFGHIJKLMNOPQRSTVUWXYZ\":\n",
    "    filter_col = [col for col in shadow_matrix if col.startswith(letter)]\n",
    "    if len(filter_col)>0:\n",
    "        shadow_matrix[letter] = shadow_matrix[filter_col].T.sum().T\n",
    "        shadow_matrix = shadow_matrix.drop(columns=filter_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some contact codes do not even appear and should be discounted.  Only look at the fields with greater than 1000 students with interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_matrix = shadow_matrix[shadow_matrix.columns[shadow_matrix[shadow_matrix>0].count()>1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(shadow_matrix,data['Unique_student_ID'],left_index=True,right_index=True).to_csv('../data/processed/contact_codes_at_application.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the data before PCA\n",
    "from sklearn.preprocessing import scale\n",
    "scaled = pd.DataFrame(scale(shadow_matrix),columns=shadow_matrix.columns.values)\n",
    "\n",
    "# implementing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=6).fit(scaled)\n",
    "pca_samples = pca.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results = vis.pca_results(scaled, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance\n",
    "vis.plot_explained_variance_ratio(pca);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biplot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.biplot(shadow_matrix, scaled, pca);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Target of WeightAtAcpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "y = data['WeightatAcpt']\n",
    "X = shadow_matrix[~y.isna()]\n",
    "y = y[~y.isna()]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.Series(reg.coef_)\n",
    "feature_importance.index = shadow_matrix.columns.values\n",
    "\n",
    "feature_importance.plot(kind='barh');\n",
    "plt.xlabel(\"Feature Weight\")\n",
    "plt.ylabel(\"Feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = vis.residual_error(X_train, X_test, y_train, y_test)\n",
    "plt.ylim(-3000,3000);\n",
    "plt.xlim(1500);\n",
    "plt.xticks(rotation=70);\n",
    "\n",
    "MAPE = 1/len(y) * abs(y-reg.predict(X.values))/y\n",
    "\n",
    "print(\"The Mean Absolute Percentage Error is: %.2f percent\" % (MAPE.sum()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good of a predictor of Enrollment is Weight???\n",
    "\n",
    "First plot the distributions against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,6))\n",
    "\n",
    "f.add_subplot(1,2,1)\n",
    "sns.stripplot(data=data,x=\"Enrolled\",y=\"WeightatAcpt\",size=1);\n",
    "plt.ylim(800,3000)\n",
    "\n",
    "f.add_subplot(1,2,2)\n",
    "sns.boxplot(data=data,x=\"Enrolled\",y=\"WeightatAcpt\",showfliers=False);\n",
    "plt.ylim(800,3000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Enrolled'].fillna(False).astype(int)\n",
    "X = shadow_matrix[~y.isna()]\n",
    "y = y.values\n",
    "X = np.array(data['WeightatAcpt'].fillna(0)).reshape(-1,1)\n",
    "\n",
    "class_names = 'AB'\n",
    "plot_colors = 'br'\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "bdt.fit(X, y)\n",
    "\n",
    "# Plot the two-class decision scores\n",
    "twoclass_output = bdt.decision_function(X)\n",
    "plot_range = (twoclass_output.min(), twoclass_output.max())\n",
    "plt.subplots(figsize=(10,6))\n",
    "\n",
    "status = ['Unlikely to Enroll','Likely to Enroll']\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    plt.hist(twoclass_output[y == i],\n",
    "             bins=20,\n",
    "             range=plot_range,\n",
    "             facecolor=c,\n",
    "             label=f'{status[i]}',\n",
    "             alpha=.5,\n",
    "             edgecolor='k',\n",
    "            density=True)\n",
    "x1, x2, y1, y2 = plt.axis()\n",
    "plt.axis((x1, x2, y1, y2 * 1.2))\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Samples')\n",
    "plt.xlabel('Score')\n",
    "plt.title('Decision Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(-0.25,-0.05)\n",
    "plt.xticks([]);\n",
    "# plt.subplots_adjust(wspace=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
